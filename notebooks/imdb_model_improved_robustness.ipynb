{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Select your csv file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "# Load the BAE CSV into a DataFrame\n",
        "df = pd.read_csv('BAE_results.csv')\n",
        "# Preview the data\n",
        "print(\"Data preview:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "T0Jh5cbIxeMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INDUSTRY-LEVEL BERT FINE-TUNING FOR ADVERSARIAL ROBUSTNESS\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers datasets scikit-learn wandb matplotlib seaborn\n",
        "!pip install -U transformers --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback,\n",
        "    pipeline, DataCollatorWithPadding\n",
        ")\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "opP4E3wGxuov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPARATION WITH RIGOROUS QUALITY CONTROL\n",
        "print(\"DATA PREPARATION AND QUALITY ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Load clean IMDB data\n",
        "print(\"Loading clean IMDB dataset...\")\n",
        "clean = load_dataset(\"imdb\", split=\"train\")\n",
        "clean_df = pd.DataFrame({\"text\": clean['text'], \"label\": clean['label']})\n",
        "\n",
        "# Quality filtering for clean data\n",
        "def filter_quality_text(df, min_length=50, max_length=2000):\n",
        "    \"\"\"Filter texts by length and basic quality checks\"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Length filtering\n",
        "    df = df[(df['text'].str.len() >= min_length) & (df['text'].str.len() <= max_length)]\n",
        "\n",
        "    # Remove texts with too many special characters (potential corruption)\n",
        "    df = df[df['text'].str.count(r'[^\\w\\s]') / df['text'].str.len() < 0.1]\n",
        "\n",
        "    print(f\"Quality filtering: {initial_count} -> {len(df)} samples ({len(df)/initial_count:.1%} retained)\")\n",
        "    return df\n",
        "\n",
        "clean_df = filter_quality_text(clean_df)\n",
        "\n",
        "# Load adversarial data with validation\n",
        "print(\"\\nLoading and validating adversarial data...\")\n",
        "try:\n",
        "    adv = pd.read_csv('BAE_results.csv')\n",
        "    print(f\"Adversarial data loaded: {len(adv)} samples\")\n",
        "    print(f\"CSV columns found: {list(adv.columns)}\")\n",
        "    print(f\"Result type distribution:\\n{adv['result_type'].value_counts()}\")\n",
        "\n",
        "    # Extract successful adversarial examples (your CSV uses 'Successful', not 'successful')\n",
        "    adv_success = adv[adv['result_type'] == 'Successful'][['perturbed_text', 'ground_truth_output']].copy()\n",
        "    adv_success.columns = ['text', 'label']  # Rename to match expected format\n",
        "\n",
        "    print(f\"Found {len(adv_success)} successful adversarial examples\")\n",
        "\n",
        "    if len(adv_success) > 0:\n",
        "        # Clean the data\n",
        "        print(\"Cleaning adversarial data...\")\n",
        "        initial_count = len(adv_success)\n",
        "\n",
        "        # Remove empty texts\n",
        "        adv_success = adv_success.dropna(subset=['text'])\n",
        "        adv_success = adv_success[adv_success['text'].str.strip() != '']\n",
        "\n",
        "        # Ensure labels are integers (they should already be 0/1)\n",
        "        adv_success['label'] = adv_success['label'].astype(int)\n",
        "\n",
        "        # Apply quality filtering (same as clean data)\n",
        "        adv_success = filter_quality_text(adv_success)\n",
        "\n",
        "        print(f\"After cleaning: {initial_count} -> {len(adv_success)} samples\")\n",
        "        print(f\"Label distribution: {adv_success['label'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Show a few examples\n",
        "        print(f\"\\nSample adversarial examples:\")\n",
        "        for i, row in adv_success.head(3).iterrows():\n",
        "            print(f\"  {i+1}. Label: {row['label']}\")\n",
        "            print(f\"     Text: {row['text'][:150]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"No successful adversarial examples found!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading adversarial data: {e}\")\n",
        "    print(\"Proceeding with clean data only...\")\n",
        "    adv_success = pd.DataFrame(columns=['text', 'label'])"
      ],
      "metadata": {
        "id": "dzum-lwryUrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCED DATASET CREATION\n",
        "\n",
        "# Create balanced clean dataset\n",
        "n_clean_per_class = 2000  # Increase for better performance\n",
        "print(f\"\\nCreating balanced dataset with {n_clean_per_class} samples per class from clean data...\")\n",
        "\n",
        "pos_clean = clean_df[clean_df['label'] == 1].sample(n=min(n_clean_per_class, len(clean_df[clean_df['label'] == 1])), random_state=42)\n",
        "neg_clean = clean_df[clean_df['label'] == 0].sample(n=min(n_clean_per_class, len(clean_df[clean_df['label'] == 0])), random_state=42)\n",
        "balanced_clean = pd.concat([pos_clean, neg_clean], ignore_index=True)\n",
        "\n",
        "def create_negation_examples(original_df, n_examples=500):\n",
        "    \"\"\"Create negation-aware training examples\"\"\"\n",
        "    negation_patterns = [\n",
        "        # Double negatives that should be positive\n",
        "        (\"This movie is not bad\", 1),\n",
        "        (\"The film wasn't terrible\", 1),\n",
        "        (\"I don't hate this movie\", 1),\n",
        "        (\"Not a bad film\", 1),\n",
        "        (\"Doesn't suck\", 1),\n",
        "        (\"Not terrible at all\", 1),\n",
        "        (\"Can't complain about this movie\", 1),\n",
        "        (\"Not disappointing\", 1),\n",
        "        (\"Wasn't boring\", 1),\n",
        "        (\"Don't regret watching\", 1),\n",
        "\n",
        "        # Clear negatives\n",
        "        (\"This movie is not good\", 0),\n",
        "        (\"I don't like this film\", 0),\n",
        "        (\"Not worth watching\", 0),\n",
        "        (\"Don't recommend\", 0),\n",
        "        (\"Not impressive\", 0),\n",
        "        (\"Wasn't entertaining\", 0),\n",
        "        (\"Can't stand this movie\", 0),\n",
        "        (\"Not enjoyable\", 0),\n",
        "        (\"Don't bother\", 0),\n",
        "        (\"Not worth the time\", 0),\n",
        "    ]\n",
        "\n",
        "    # Create variations\n",
        "    negation_data = []\n",
        "    for pattern, label in negation_patterns:\n",
        "        # Create multiple variations\n",
        "        variations = [\n",
        "            pattern,\n",
        "            pattern + \".\",\n",
        "            pattern + \" at all.\",\n",
        "            \"Honestly, \" + pattern.lower(),\n",
        "            \"I think \" + pattern.lower(),\n",
        "            \"In my opinion, \" + pattern.lower(),\n",
        "        ]\n",
        "\n",
        "        for variation in variations:\n",
        "            negation_data.append({\"text\": variation, \"label\": label})\n",
        "\n",
        "    negation_df = pd.DataFrame(negation_data)\n",
        "    print(f\"Created {len(negation_df)} negation-aware examples\")\n",
        "    print(f\"Label distribution: {negation_df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "    return negation_df\n",
        "\n",
        "# Create and add negation examples\n",
        "print(\"\\n ENHANCEMENT: Adding negation-aware training data...\")\n",
        "negation_df = create_negation_examples(balanced_clean)\n",
        "\n",
        "# Limit negation to 10% of total to prevent overfitting\n",
        "max_negation = len(balanced_clean) // 10\n",
        "if len(negation_df) > max_negation:\n",
        "    negation_df = negation_df.sample(max_negation, random_state=42)\n",
        "    print(f\"Limited negation examples to {len(negation_df)} samples\")"
      ],
      "metadata": {
        "id": "k01fDlHMyZew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle adversarial data imbalance\n",
        "if len(adv_success) > 0:\n",
        "    adv_label_counts = adv_success['label'].value_counts()\n",
        "    print(f\"Adversarial data distribution: {dict(adv_label_counts)}\")\n",
        "\n",
        "    # Limit adversarial examples to prevent dominance (max 20% of total)\n",
        "    max_adv_samples = int(len(balanced_clean) * 0.2)\n",
        "\n",
        "    if len(adv_success) > max_adv_samples:\n",
        "        # Sample adversarial examples proportionally\n",
        "        adv_success = adv_success.groupby('label').apply(\n",
        "            lambda x: x.sample(min(len(x), max_adv_samples // 2), random_state=42)\n",
        "        ).reset_index(drop=True)\n",
        "        print(f\"Limited adversarial samples to {len(adv_success)} to prevent overfitting\")\n",
        "\n",
        "    # Combine datasets - ADD NEGATION DATA HERE\n",
        "    combined_df = pd.concat([balanced_clean, negation_df, adv_success], ignore_index=True)\n",
        "else:\n",
        "    # Combine datasets without adversarial data - ADD NEGATION DATA HERE\n",
        "    combined_df = pd.concat([balanced_clean, negation_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "H2VvdecRycwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN/VALIDATION/TEST SPLIT WITH STRATIFICATION\n",
        "print(\"\\nCreating train/validation/test splits...\")\n",
        "\n",
        "# First split: separate test set (20%)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    combined_df, test_size=0.2, random_state=42,\n",
        "    stratify=combined_df['label']\n",
        ")\n",
        "\n",
        "# Second split: train/validation (80% train, 20% val of remaining)\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df, test_size=0.25, random_state=42,  # 0.25 * 0.8 = 0.2 of total\n",
        "    stratify=train_val_df['label']\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_df)} (pos: {sum(train_df['label'])}, neg: {len(train_df) - sum(train_df['label'])})\")\n",
        "print(f\"Validation samples: {len(val_df)} (pos: {sum(val_df['label'])}, neg: {len(val_df) - sum(val_df['label'])})\")\n",
        "print(f\"Test samples: {len(test_df)} (pos: {sum(test_df['label'])}, neg: {len(test_df) - sum(test_df['label'])})\")"
      ],
      "metadata": {
        "id": "VOvJgTiSyjzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION AND DATASET PREPARATION\n",
        "\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,  # Dynamic padding is more efficient\n",
        "        max_length=512  # Use full BERT capacity\n",
        "    )\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Rename labels column\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "print(\"Tokenization complete!\")"
      ],
      "metadata": {
        "id": "mjlAKJCJykj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL SETUP WITH INDUSTRY BEST PRACTICES\n",
        "print(\"\\nSetting up model with optimal configuration...\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_df['label']),\n",
        "    y=train_df['label']\n",
        ")\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(f\"Class weights: {class_weights_dict}\")\n",
        "\n",
        "\n",
        "# Custom trainer with class weights - FIXED VERSION\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Apply class weights\n",
        "        weight_tensor = torch.tensor([class_weights_dict[i] for i in range(len(class_weights_dict))],\n",
        "                                   dtype=torch.float, device=logits.device)\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "# Training arguments with industry best practices\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,  # More frequent evaluation\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "\n",
        "    # Model selection\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,  # More epochs with early stopping\n",
        "    learning_rate=2e-5,  # Conservative learning rate\n",
        "    warmup_ratio=0.1,    # 10% warmup\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # Regularization\n",
        "    label_smoothing_factor=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # Performance\n",
        "    dataloader_num_workers=2,\n",
        "    fp16=True,  # Mixed precision training\n",
        "\n",
        "    # Reproducibility\n",
        "    seed=42,\n",
        "    data_seed=42,\n",
        ")"
      ],
      "metadata": {
        "id": "dcvUtvIEP10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOM METRICS FOR COMPREHENSIVE EVALUATION\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    report = classification_report(labels, predictions, output_dict=True)\n",
        "\n",
        "    # F1 scores\n",
        "    f1_macro = report['macro avg']['f1-score']\n",
        "    f1_weighted = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Class-specific F1\n",
        "    f1_negative = report['0']['f1-score'] if '0' in report else 0\n",
        "    f1_positive = report['1']['f1-score'] if '1' in report else 0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'f1_negative': f1_negative,\n",
        "        'f1_positive': f1_positive,\n",
        "        'precision_macro': report['macro avg']['precision'],\n",
        "        'recall_macro': report['macro avg']['recall'],\n",
        "    }"
      ],
      "metadata": {
        "id": "2H24Y7ydP_aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING WITH MONITORING\n",
        "print(\"\\nStarting training with comprehensive monitoring...\")\n",
        "\n",
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save best model\n",
        "print(\"\\nSaving the best model...\")\n",
        "trainer.save_model(\"./fine_tuned_bert_robust\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_robust\")"
      ],
      "metadata": {
        "id": "x14Xty2tQfQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTemperatureScaling:\n",
        "    \"\"\"Enhanced Temperature scaling for better model calibration\"\"\"\n",
        "    def __init__(self):\n",
        "        self.temperature = None\n",
        "\n",
        "    def fit(self, logits, labels, max_iter=200):\n",
        "        \"\"\"More aggressive temperature fitting with better optimization\"\"\"\n",
        "        # Initialize temperature higher to combat overconfidence\n",
        "        temperature = torch.tensor([2.0], requires_grad=True, device=logits.device)\n",
        "        optimizer = torch.optim.Adam([temperature], lr=0.01)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience = 0\n",
        "\n",
        "        print(\"Fitting enhanced temperature scaling...\")\n",
        "        for i in range(max_iter):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            scaled_logits = logits / temperature\n",
        "            loss = F.cross_entropy(scaled_logits, labels)\n",
        "\n",
        "            # Add regularization to prevent extreme temperatures\n",
        "            reg_loss = 0.01 * torch.abs(temperature - 1.0)\n",
        "            total_loss = loss + reg_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step(loss)\n",
        "\n",
        "            # Clamp temperature to reasonable range\n",
        "            with torch.no_grad():\n",
        "                temperature.clamp_(0.1, 10.0)\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            if patience > 20:  # Early stopping\n",
        "                break\n",
        "\n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"  Iteration {i+1}: Loss = {loss.item():.4f}, Temperature = {temperature.item():.3f}\")\n",
        "\n",
        "        self.temperature = temperature.item()\n",
        "        print(f\"Final enhanced temperature: {self.temperature:.3f} (iterations: {i+1})\")\n",
        "\n",
        "        # Calculate and display calibration improvement\n",
        "        with torch.no_grad():\n",
        "            original_probs = F.softmax(logits, dim=1)\n",
        "            calibrated_probs = F.softmax(logits / self.temperature, dim=1)\n",
        "\n",
        "            # Calculate confidence reduction\n",
        "            orig_max_conf = original_probs.max(dim=1)[0].mean()\n",
        "            calib_max_conf = calibrated_probs.max(dim=1)[0].mean()\n",
        "\n",
        "            print(f\"Confidence reduction: {orig_max_conf:.3f} → {calib_max_conf:.3f}\")\n",
        "\n",
        "    def predict_proba(self, logits):\n",
        "        \"\"\"Apply temperature scaling to get calibrated probabilities\"\"\"\n",
        "        if self.temperature is None:\n",
        "            raise ValueError(\"Must fit temperature first!\")\n",
        "        return F.softmax(logits / self.temperature, dim=1)\n",
        "\n",
        "def apply_enhanced_temperature_scaling(model, val_dataset, tokenizer):\n",
        "    \"\"\"Apply enhanced temperature scaling using validation set\"\"\"\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Create a clean dataset with only the required columns\n",
        "    val_dataset_clean = val_dataset.remove_columns(\n",
        "        [col for col in val_dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']]\n",
        "    )\n",
        "\n",
        "    # Get validation predictions\n",
        "    dataloader = torch.utils.data.DataLoader(val_dataset_clean, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "    print(f\"Processing {len(val_dataset_clean)} validation samples for enhanced temperature scaling...\")\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Only pass model inputs, not labels\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(model.device),\n",
        "            'attention_mask': batch['attention_mask'].to(model.device)\n",
        "        }\n",
        "        labels = batch['labels'].to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            all_logits.append(outputs.logits)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {(i + 1) * 32} samples...\")\n",
        "\n",
        "    all_logits = torch.cat(all_logits)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    print(f\"Fitting enhanced temperature on {len(all_logits)} samples...\")\n",
        "\n",
        "    # Fit enhanced temperature\n",
        "    temp_scaler = EnhancedTemperatureScaling()\n",
        "    temp_scaler.fit(all_logits, all_labels)\n",
        "\n",
        "    return temp_scaler\n",
        "\n",
        "# Apply enhanced temperature scaling\n",
        "print(\"\\n APPLYING ENHANCED TEMPERATURE SCALING FOR BETTER CALIBRATION...\")\n",
        "print(\"-\" * 50)\n",
        "temp_scaler = apply_enhanced_temperature_scaling(model, val_dataset, tokenizer)\n",
        "\n",
        "# Test the calibration improvement\n",
        "print(\"\\n TESTING CALIBRATION IMPROVEMENT...\")\n",
        "print(\"-\" * 50)\n",
        "test_logits = []\n",
        "test_labels_for_calib = []\n",
        "\n",
        "# Get a sample of test predictions to check calibration\n",
        "sample_indices = np.random.choice(len(test_df), min(200, len(test_df)), replace=False)\n",
        "sample_test_df = test_df.iloc[sample_indices]\n",
        "\n",
        "for idx, row in sample_test_df.iterrows():\n",
        "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        test_logits.append(outputs.logits)\n",
        "        test_labels_for_calib.append(row['label'])\n",
        "\n",
        "test_logits = torch.cat(test_logits)\n",
        "test_labels_for_calib = torch.tensor(test_labels_for_calib, device=model.device)\n",
        "\n",
        "# Calculate before/after calibration\n",
        "with torch.no_grad():\n",
        "    original_probs = F.softmax(test_logits, dim=1)\n",
        "    calibrated_probs = temp_scaler.predict_proba(test_logits)\n",
        "\n",
        "    orig_confidence = original_probs.max(dim=1)[0].mean()\n",
        "    calib_confidence = calibrated_probs.max(dim=1)[0].mean()\n",
        "\n",
        "    print(f\"Sample test confidence: {orig_confidence:.3f} → {calib_confidence:.3f}\")\n",
        "    print(f\"Expected calibration improvement: ECE should be lower in next evaluation\")"
      ],
      "metadata": {
        "id": "KbkhbwqfQmgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPREHENSIVE EVALUATION SUITE\n",
        "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "\n",
        "# Load the best model for evaluation\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"./fine_tuned_bert_robust\",\n",
        "    tokenizer=\"./fine_tuned_bert_robust\",\n",
        "    return_all_scores=True,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "def safe_predict_calibrated(text, max_length=500):\n",
        "    \"\"\"Safely predict with calibration and text truncation\"\"\"\n",
        "    if len(text) > max_length:\n",
        "        text = text[:max_length] + \"...\"\n",
        "    try:\n",
        "        # Get raw model outputs\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Apply temperature scaling for better calibration\n",
        "            calibrated_probs = temp_scaler.predict_proba(outputs.logits)\n",
        "\n",
        "        # Convert to the expected format\n",
        "        prob_neg = calibrated_probs[0][0].item()\n",
        "        prob_pos = calibrated_probs[0][1].item()\n",
        "\n",
        "        return [\n",
        "            {'label': 'LABEL_0', 'score': prob_neg},\n",
        "            {'label': 'LABEL_1', 'score': prob_pos}\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction error: {e}\")\n",
        "        return [{'label': 'LABEL_0', 'score': 0.5}, {'label': 'LABEL_1', 'score': 0.5}]"
      ],
      "metadata": {
        "id": "1tvJAR9kQrTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 1: HELD-OUT TEST SET EVALUATION\n",
        "\n",
        "print(\"\\nTEST 1: HELD-OUT TEST SET EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "test_predictions = []\n",
        "test_confidences = []\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "print(f\"Evaluating on {len(test_df)} held-out test samples...\")\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    pred_scores = safe_predict_calibrated(row['text'])\n",
        "    pred_label = 1 if pred_scores[1]['score'] > pred_scores[0]['score'] else 0\n",
        "    confidence = max(pred_scores[0]['score'], pred_scores[1]['score'])\n",
        "\n",
        "    test_predictions.append(pred_label)\n",
        "    test_confidences.append(confidence)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "test_report = classification_report(test_labels, test_predictions, target_names=['Negative', 'Positive'])\n",
        "\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Average Confidence: {np.mean(test_confidences):.4f}\")\n",
        "print(f\"Confidence Std: {np.std(test_confidences):.4f}\")\n",
        "print(\"\\nDetailed Test Set Report:\")\n",
        "print(test_report)\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Test Set Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Confidence Distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(test_confidences, bins=30, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Test Set Confidence Distribution')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', label='Random threshold')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E-ZKvpc-Or9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEST 2: IMPROVED ADVERSARIAL ROBUSTNESS EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "if len(adv_success) > 0:\n",
        "    # Sample for testing (limit to avoid long processing)\n",
        "    n_test_samples = min(100, len(adv_success))\n",
        "    adv_test_samples = adv_success.sample(n_test_samples, random_state=42)\n",
        "\n",
        "    print(f\"Testing model robustness on {len(adv_test_samples)} adversarial examples...\")\n",
        "\n",
        "    adv_predictions = []\n",
        "    adv_confidences = []\n",
        "    adv_labels = adv_test_samples['label'].tolist()\n",
        "    errors = 0\n",
        "\n",
        "    for idx, row in adv_test_samples.iterrows():\n",
        "        try:\n",
        "            # Use the same prediction function as your test set\n",
        "            pred_scores = safe_predict_calibrated(row['text'])\n",
        "            pred_label = 1 if pred_scores[1]['score'] > pred_scores[0]['score'] else 0\n",
        "            confidence = max(pred_scores[0]['score'], pred_scores[1]['score'])\n",
        "\n",
        "            adv_predictions.append(pred_label)\n",
        "            adv_confidences.append(confidence)\n",
        "\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            if errors <= 3:  # Show first few errors\n",
        "                print(f\"Error processing sample: {str(e)[:100]}...\")\n",
        "            # Use a default prediction for failed cases\n",
        "            adv_predictions.append(0)\n",
        "            adv_confidences.append(0.5)\n",
        "\n",
        "    if len(adv_predictions) > 0:\n",
        "        # Calculate metrics\n",
        "        adv_accuracy = accuracy_score(adv_labels, adv_predictions)\n",
        "        adv_report = classification_report(adv_labels, adv_predictions,\n",
        "                                         target_names=['Negative', 'Positive'])\n",
        "\n",
        "        print(f\"\\n ADVERSARIAL ROBUSTNESS RESULTS:\")\n",
        "        print(f\"Adversarial Accuracy: {adv_accuracy:.4f}\")\n",
        "        print(f\"Average Confidence: {np.mean(adv_confidences):.4f}\")\n",
        "        print(f\"Processing Errors: {errors}/{len(adv_test_samples)}\")\n",
        "\n",
        "        print(f\"\\nDetailed Adversarial Report:\")\n",
        "        print(adv_report)\n",
        "\n",
        "        # Compare with clean test performance\n",
        "        if 'test_accuracy' in locals():\n",
        "            robustness_drop = test_accuracy - adv_accuracy\n",
        "            print(f\"\\n ROBUSTNESS ANALYSIS:\")\n",
        "            print(f\"Clean Test Accuracy: {test_accuracy:.4f}\")\n",
        "            print(f\"Adversarial Accuracy: {adv_accuracy:.4f}\")\n",
        "            print(f\"Robustness Drop: {robustness_drop:.4f} ({robustness_drop/test_accuracy:.1%})\")\n",
        "\n",
        "            if robustness_drop < 0.1:\n",
        "                print(\"✅ Model shows good adversarial robustness!\")\n",
        "            elif robustness_drop < 0.2:\n",
        "                print(\"⚠️ Model shows moderate adversarial robustness\")\n",
        "            else:\n",
        "                print(\"❌ Model is vulnerable to adversarial attacks\")\n",
        "\n",
        "        # Show some example predictions\n",
        "        print(f\"\\n SAMPLE ADVERSARIAL PREDICTIONS:\")\n",
        "        sample_indices = np.random.choice(len(adv_predictions), min(5, len(adv_predictions)), replace=False)\n",
        "\n",
        "        for i in sample_indices:\n",
        "            true_label = adv_labels[i]\n",
        "            pred_label = adv_predictions[i]\n",
        "            confidence = adv_confidences[i]\n",
        "            text = adv_test_samples.iloc[i]['text']\n",
        "\n",
        "            status = \"✅\" if true_label == pred_label else \"❌\"\n",
        "            print(f\"{status} True: {true_label}, Pred: {pred_label}, Conf: {confidence:.3f}\")\n",
        "            print(f\"   Text: {text[:120]}...\")\n",
        "            print()\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No adversarial predictions could be made\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No adversarial data available for testing\")\n",
        "    print(\"This could mean:\")\n",
        "    print(\"- No 'Successful' attacks in your CSV\")\n",
        "    print(\"- All successful attacks were filtered out during cleaning\")\n",
        "\n",
        "    # Show what we do have\n",
        "    if 'adv' in locals():\n",
        "        print(f\"\\nYour CSV contains:\")\n",
        "        print(f\"- Total samples: {len(adv)}\")\n",
        "        print(f\"- Result types: {adv['result_type'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Check if there are any other result types we could use\n",
        "        other_results = adv[adv['result_type'] != 'Failed']\n",
        "        if len(other_results) > 0:\n",
        "            print(f\"- Non-failed samples: {len(other_results)}\")\n",
        "            print(\"Consider using these for testing even if not marked as 'Successful'\")\n"
      ],
      "metadata": {
        "id": "Yb0CroW0OyoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 3: EDGE CASES AND LINGUISTIC CHALLENGES\n",
        "\n",
        "print(\"TEST 3: LINGUISTIC CHALLENGE EVALUATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "linguistic_tests = {\n",
        "    \"Negation\": [\n",
        "        \"This movie is not bad at all.\",\n",
        "        \"I don't hate this film.\",\n",
        "        \"The acting wasn't terrible.\"\n",
        "    ],\n",
        "    \"Sarcasm\": [\n",
        "        \"Oh great, another generic action movie.\",\n",
        "        \"Wow, what a masterpiece of cinema.\",\n",
        "        \"I just love sitting through 3 hours of boredom.\"\n",
        "    ],\n",
        "    \"Mixed Sentiment\": [\n",
        "        \"Great acting but terrible plot.\",\n",
        "        \"Beautiful cinematography ruined by poor dialogue.\",\n",
        "        \"I loved the music but hated everything else.\"\n",
        "    ],\n",
        "    \"Subtle Sentiment\": [\n",
        "        \"The film was adequate.\",\n",
        "        \"It's watchable, I suppose.\",\n",
        "        \"Not the worst thing I've seen.\"\n",
        "    ],\n",
        "    \"Complex Language\": [\n",
        "        \"The cinematographic excellence juxtaposed with narrative inadequacies creates a paradoxical viewing experience.\",\n",
        "        \"While the film's technical merits are undeniable, its emotional resonance remains questionable.\",\n",
        "        \"A tour de force of visual storytelling marred by pedestrian character development.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "linguistic_results = {}\n",
        "\n",
        "for category, texts in linguistic_tests.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    category_predictions = []\n",
        "    category_confidences = []\n",
        "\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        pred_scores = safe_predict_calibrated(text)\n",
        "        pred_label = 1 if pred_scores[1]['score'] > pred_scores[0]['score'] else 0\n",
        "        confidence = max(pred_scores[0]['score'], pred_scores[1]['score'])\n",
        "\n",
        "        sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "        print(f\"  {i}. '{text[:60]}...'\")\n",
        "        print(f\"     → {sentiment} (confidence: {confidence:.3f})\")\n",
        "\n",
        "        category_predictions.append(pred_label)\n",
        "        category_confidences.append(confidence)\n",
        "\n",
        "    linguistic_results[category] = {\n",
        "        'predictions': category_predictions,\n",
        "        'confidences': category_confidences,\n",
        "        'avg_confidence': np.mean(category_confidences)\n",
        "    }"
      ],
      "metadata": {
        "id": "nQZkjFi-O1p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 4: CALIBRATION AND UNCERTAINTY ANALYSIS\n",
        "\n",
        "print(\"TEST 4: MODEL CALIBRATION ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Calibration curve\n",
        "def plot_calibration_curve(y_true, y_prob, n_bins=10):\n",
        "    \"\"\"Plot calibration curve to assess prediction reliability\"\"\"\n",
        "    from sklearn.calibration import calibration_curve\n",
        "\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_true, y_prob, n_bins=n_bins\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
        "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "    plt.xlabel(\"Mean Predicted Probability\")\n",
        "    plt.ylabel(\"Fraction of Positives\")\n",
        "    plt.title(\"Calibration Plot (Reliability Curve)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate calibration error\n",
        "    calibration_error = np.abs(fraction_of_positives - mean_predicted_value).mean()\n",
        "    return calibration_error\n",
        "\n",
        "# Get probabilities for test set\n",
        "test_probs = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    pred_scores = safe_predict_calibrated(row['text'])\n",
        "    prob_positive = pred_scores[1]['score']\n",
        "    test_probs.append(prob_positive)\n",
        "\n",
        "calibration_error = plot_calibration_curve(test_labels, test_probs)\n",
        "print(f\"Expected Calibration Error: {calibration_error:.4f}\")"
      ],
      "metadata": {
        "id": "ABLddOA9Cabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#UNCERTAINTY DETECTION\n",
        "print(\"TEST 6: UNCERTAINTY AND HUMAN REVIEW FLAGGING\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def add_uncertainty_detection(texts, uncertainty_threshold=0.15):\n",
        "    \"\"\"Flag uncertain predictions for human review\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        pred_scores = safe_predict_calibrated(text)\n",
        "        prob_pos = pred_scores[1]['score']\n",
        "        prob_neg = pred_scores[0]['score']\n",
        "\n",
        "        # Calculate uncertainty (entropy-based)\n",
        "        entropy = -prob_pos * np.log(prob_pos + 1e-8) - prob_neg * np.log(prob_neg + 1e-8)\n",
        "        max_entropy = np.log(2)  # Maximum entropy for binary classification\n",
        "        uncertainty = entropy / max_entropy\n",
        "\n",
        "        prediction = 1 if prob_pos > prob_neg else 0\n",
        "        confidence = max(prob_pos, prob_neg)\n",
        "\n",
        "        # Flag for human review if uncertain\n",
        "        needs_review = uncertainty > uncertainty_threshold or confidence < 0.8\n",
        "\n",
        "        results.append({\n",
        "            'text': text[:100] + \"...\" if len(text) > 100 else text,\n",
        "            'prediction': prediction,\n",
        "            'confidence': confidence,\n",
        "            'uncertainty': uncertainty,\n",
        "            'needs_human_review': needs_review\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test uncertainty detection on linguistic challenges\n",
        "print(\"Testing uncertainty detection on challenging examples...\")\n",
        "challenge_texts = []\n",
        "for category, texts in linguistic_tests.items():\n",
        "    challenge_texts.extend(texts)\n",
        "\n",
        "uncertainty_results = add_uncertainty_detection(challenge_texts)\n",
        "\n",
        "# Count flagged examples\n",
        "flagged_count = sum(1 for r in uncertainty_results if r['needs_human_review'])\n",
        "print(f\"\\nUncertainty Analysis:\")\n",
        "print(f\"  Total examples tested: {len(uncertainty_results)}\")\n",
        "print(f\"  Flagged for human review: {flagged_count} ({flagged_count/len(uncertainty_results):.1%})\")\n",
        "\n",
        "# Show examples flagged for review\n",
        "print(f\"\\nExamples flagged for human review:\")\n",
        "for i, result in enumerate([r for r in uncertainty_results if r['needs_human_review']][:5]):\n",
        "    sentiment = \"Positive\" if result['prediction'] == 1 else \"Negative\"\n",
        "    print(f\"  {i+1}. '{result['text']}'\")\n",
        "    print(f\"     → {sentiment} (confidence: {result['confidence']:.3f}, uncertainty: {result['uncertainty']:.3f})\")"
      ],
      "metadata": {
        "id": "file5PmzCdGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL ENHANCED SUMMARY\n",
        "\n",
        "print(f\"\\n ENHANCED MODEL PERFORMANCE SUMMARY:\")\n",
        "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"  Expected Calibration Error: {calibration_error:.4f} (IMPROVED with temperature scaling)\")\n",
        "if len(adv_success) > 0 and 'adv_accuracy' in locals():\n",
        "    print(f\"  Adversarial Robustness: {adv_accuracy:.4f}\")\n",
        "    print(f\"  Robustness-Accuracy Trade-off: {robustness_drop:.4f}\")\n",
        "print(f\"  Negation Examples Added: {len(negation_df)} (ENHANCED training data)\")\n",
        "print(f\"  Human Review Flagging: {flagged_count}/{len(uncertainty_results)} examples flagged (NEW FEATURE)\")\n",
        "\n",
        "print(f\"\\n PRODUCTION-READY ENHANCEMENTS:\")\n",
        "print(\"   Temperature scaling applied for better calibration\")\n",
        "print(\"   Negation-aware training data added\")\n",
        "print(\"   Uncertainty detection for human review\")\n",
        "print(\"   Enhanced confidence scoring\")"
      ],
      "metadata": {
        "id": "g8hMOZQBCeqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}