{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install transformers textattack datasets --quiet\n",
        "\n",
        "# Imports\n",
        "import os, random, numpy as np, torch\n",
        "from copy import deepcopy\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack import Attacker, AttackArgs\n",
        "\n",
        "# Import recipes - fixed import paths\n",
        "from textattack.attack_recipes import (\n",
        "    TextFoolerJin2019,\n",
        "    DeepWordBugGao2018,\n",
        "    PWWSRen2019,\n",
        "    BAEGarg2019,\n",
        ")\n",
        "\n",
        "# Set seed\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on device:\", device)\n",
        "\n",
        "# Load & wrap model\n",
        "model_name = \"textattack/bert-base-uncased-imdb\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "# Dataset - limit to smaller subset for testing\n",
        "dataset = HuggingFaceDataset(\"imdb\", split=\"test\")\n",
        "\n",
        "# Attack arguments\n",
        "base_args = AttackArgs(\n",
        "    num_examples=5,  # Reduced for faster testing\n",
        "    random_seed=seed,\n",
        "    shuffle=False,\n",
        "    disable_stdout=False,  # Enable to see progress\n",
        "    log_to_csv=None,\n",
        "    parallel=False  # Disable parallel processing to avoid issues\n",
        ")\n",
        "\n",
        "# Define attacks - fixed class references\n",
        "attacks = {\n",
        "    \"TextFooler\": TextFoolerJin2019.build(wrapper),\n",
        "    \"DeepWordBug\": DeepWordBugGao2018.build(wrapper),\n",
        "    \"PWWS\": PWWSRen2019.build(wrapper),\n",
        "    \"BAE\": BAEGarg2019.build(wrapper),\n",
        "}\n",
        "\n",
        "# Run attacks with error handling and CSV saving\n",
        "results = {}\n",
        "for name, attack in attacks.items():\n",
        "    print(f\"Running {name}...\")\n",
        "    try:\n",
        "        args = deepcopy(base_args)\n",
        "        # Set CSV filename for this attack\n",
        "        csv_filename = f\"{name.replace(' ', '_').replace('(', '').replace(')', '')}_results.csv\"\n",
        "        args.log_to_csv = csv_filename\n",
        "\n",
        "        attacker = Attacker(attack, dataset, args)\n",
        "        result = attacker.attack_dataset()\n",
        "        results[name] = result\n",
        "        print(f\"{name} completed successfully â€” results saved to {csv_filename}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error running {name}: {str(e)}\")\n",
        "        print(f\"Skipping {name} and continuing...\\n\")\n",
        "        continue\n",
        "\n",
        "print(\"All attacks completed!\")\n",
        "print(\"Results summary:\")\n",
        "for name, result in results.items():\n",
        "    if result:\n",
        "        print(f\"{name}: {len(result)} examples processed\")"
      ],
      "metadata": {
        "id": "JRPDcsSVvgth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all CSV files in the current directory\n",
        "[file for file in os.listdir() if file.endswith(\".csv\")]\n"
      ],
      "metadata": {
        "id": "jUHUEowm54gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('TextFooler_results.csv')\n",
        "files.download('DeepWordBug_results.csv')\n",
        "files.download('PWWS_results.csv')\n",
        "files.download('BAE_results.csv')"
      ],
      "metadata": {
        "id": "FWVarZ66k9Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select your csv files\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "QaftniBIVYqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q sentence-transformers transformers\n",
        "\n",
        "# Imports\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load models\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_model.eval()\n",
        "\n",
        "# Load CSVs\n",
        "textfooler = pd.read_csv(\"TextFooler_results.csv\")\n",
        "deepwordbug = pd.read_csv(\"DeepWordBug_results.csv\")\n",
        "pwws = pd.read_csv(\"PWWS_results.csv\")\n",
        "bae = pd.read_csv(\"BAE_results.csv\")\n",
        "\n",
        "methods = {\n",
        "    \"TextFooler\": textfooler,\n",
        "    \"DeepWordBug\": deepwordbug,\n",
        "    \"PWWS\": pwws,\n",
        "    \"BAE\": bae\n",
        "\n",
        "}\n",
        "\n",
        "# --- Metric Functions ---\n",
        "\n",
        "def success_rate(df):\n",
        "    df = df[df['result_type'] != 'Skipped']\n",
        "    return (df['result_type'] == 'Successful').mean()\n",
        "\n",
        "def avg_words_perturbed(df, result_type):\n",
        "    filtered = df[df['result_type'] != 'Skipped']\n",
        "    if result_type == 'Successful':\n",
        "        filtered = filtered[filtered['result_type'] == 'Successful']\n",
        "    elif result_type == 'Failed':\n",
        "        filtered = filtered[filtered['result_type'] != 'Successful']\n",
        "    counts = filtered['perturbed_text'].apply(lambda x: str(x).count('[['))\n",
        "    return counts.mean()\n",
        "\n",
        "def semantic_similarity(original_texts, perturbed_texts):\n",
        "    embeddings1 = semantic_model.encode(original_texts.tolist(), convert_to_tensor=True)\n",
        "    embeddings2 = semantic_model.encode(perturbed_texts.tolist(), convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(embeddings1, embeddings2)\n",
        "    return similarities.diag().cpu().numpy().mean()\n",
        "\n",
        "def avg_fluency_score(texts):\n",
        "    scores = []\n",
        "    for t in texts:\n",
        "        encodings = gpt2_tokenizer(str(t), return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            outputs = gpt2_model(**encodings, labels=encodings[\"input_ids\"])\n",
        "            log_likelihood = outputs.loss.item()\n",
        "            scores.append(-log_likelihood)  # Higher is better (less perplexity)\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# --- Final Output ---\n",
        "\n",
        "for name, df in methods.items():\n",
        "    df = df[df['result_type'] != 'Skipped']\n",
        "    success_df = df[df['result_type'] == 'Successful']\n",
        "    fail_df = df[df['result_type'] != 'Successful']\n",
        "\n",
        "    sr = success_rate(df) * 100\n",
        "    avg_success = avg_words_perturbed(df, 'Successful')\n",
        "    avg_fail = avg_words_perturbed(df, 'Failed')\n",
        "\n",
        "    if not success_df.empty:\n",
        "        sim_score = semantic_similarity(success_df['original_text'], success_df['perturbed_text'])\n",
        "        fluency_score = avg_fluency_score(success_df['perturbed_text'])\n",
        "    else:\n",
        "        sim_score = float('nan')\n",
        "        fluency_score = float('nan')\n",
        "\n",
        "    print(f\"{name} Success Rate: {sr:.2f}%\")\n",
        "    print(f\"{name} Avg. Words Perturbed (Successes): {avg_success:.2f}\")\n",
        "    print(f\"{name} Avg. Words Perturbed (Fails): {avg_fail:.2f}\")\n",
        "    print(f\"{name} Avg. Semantic Similarity (Successes): {sim_score:.3f}\")\n",
        "    print(f\"{name} Avg. Fluency Score (GPT-2) (Successes): {fluency_score:.2f}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "l-sonWDlDwux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}